# Voice Transcription Integration

This document describes the direct integration of voice transcription capabilities into the main backend API.

## üéØ What's Been Integrated

The Voice Transcribe API has been directly integrated into your main backend (`api/server.py`) with the following new capabilities:

### New Endpoints

1. **`POST /voice_transcribe`** - Transcribe audio files to structured conversation format
2. **`POST /voice_infer`** - Voice-based clinical inference (transcribe + analyze)
3. **`POST /multimodal_voice_infer`** - Full multimodal inference with voice + image

### New Components

- **`core/voice_transcription.py`** - Voice transcription service with WhisperX integration
- **Updated `requirements.txt`** - Includes all audio processing dependencies
- **Enhanced health endpoint** - Shows voice transcription model status

## üöÄ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Start the Server

```bash
uvicorn api.server:app --host 0.0.0.0 --port 8000
```

### 3. Test the Integration

```bash
python test_voice_integration.py
```

## üìã API Endpoints

### Voice Transcription Only

```bash
curl -X POST "http://localhost:8000/voice_transcribe" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@audio_file.wav" \
  -F "description=Patient consultation about symptoms"
```

**Response:**
```json
{
  "session_id": "uuid-string",
  "conversation": [
    {
      "description": "Patient consultation about symptoms",
      "utterances": [
        {
          "speaker": "patient",
          "text": "I have a headache and feel dizzy",
          "timestamp": 0.0,
          "confidence": 0.9
        },
        {
          "speaker": "doctor",
          "text": "How long have you had these symptoms?",
          "timestamp": 3.2,
          "confidence": 0.9
        }
      ]
    }
  ],
  "timestamp": "2024-01-01T12:00:00"
}
```

### Voice-Based Clinical Inference

```bash
curl -X POST "http://localhost:8000/voice_infer" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@audio_file.wav" \
  -F "patient_id=patient_001" \
  -F "description=Patient consultation"
```

**Response:** Full clinical analysis including:
- Transcription results
- Extracted symptoms and findings
- RAG-based advisory
- Suggested follow-up questions
- Confidence scores and fusion results

### Multimodal Voice + Image Inference

```bash
curl -X POST "http://localhost:8000/multimodal_voice_infer" \
  -H "Content-Type: multipart/form-data" \
  -F "audio_file=@conversation.wav" \
  -F "image_file=@xray.jpg" \
  -F "patient_id=patient_001" \
  -F "description=Patient consultation with X-ray"
```

## üîß Configuration

### Environment Variables

```bash
# Optional: For enhanced speaker diarization
export HUGGINGFACE_HUB_TOKEN="your_token_here"

# Existing backend configuration
export GROQ_API_KEY="your_groq_key"
export EHR_JSON="ehr_with_images.json"
export RAG_PERSIST_DIR="./rag_store"
```

### Model Configuration

The integration uses WhisperX "base" model by default. To change the model size, edit `core/voice_transcription.py`:

```python
# Change this line in the _load_models method:
self.whisperx_model = whisperx.load_model("base", device="cpu", compute_type="int8")
# Options: "tiny", "base", "small", "medium", "large"
```

## üé§ Supported Audio Formats

- WAV
- MP3
- M4A
- FLAC
- OGG
- And other formats supported by FFmpeg

## üîç Health Check

Check the status of voice transcription models:

```bash
curl http://localhost:8000/health
```

**Response includes:**
```json
{
  "status": "ok",
  "voice_transcription": {
    "whisperx_model_loaded": true,
    "diarization_model_loaded": true,
    "alignment_model_loaded": true
  }
}
```

## üß™ Testing

### Automated Testing

Run the test script to verify everything is working:

```bash
python test_voice_integration.py
```

### Manual Testing

1. **Test with sample audio:**
   - Place a test audio file (wav/mp3) in the project root
   - Run the test script
   - Check the API documentation at `http://localhost:8000/docs`

2. **Test with real conversation:**
   - Record a patient-doctor conversation
   - Upload via the API endpoints
   - Verify transcription and clinical analysis

## üö® Troubleshooting

### Common Issues

1. **"No module named 'torch'"**
   ```bash
   pip install torch torchaudio
   ```

2. **"No module named 'pyaudio'"**
   - **macOS**: `brew install portaudio`
   - **Ubuntu**: `sudo apt install portaudio19-dev`
   - **Windows**: Download from http://www.portaudio.com/

3. **WhisperX model loading errors**
   - Ensure sufficient disk space (models download on first use)
   - Check internet connection for model downloads

4. **Speaker diarization not working**
   - Set `HUGGINGFACE_HUB_TOKEN` environment variable
   - Or use without token (limited functionality)

5. **Audio format not supported**
   - Ensure FFmpeg is installed
   - Convert audio to supported format

### Performance Tips

- Use shorter audio files for faster processing
- Consider using "tiny" model for real-time applications
- Enable GPU acceleration if available
- Implement caching for repeated transcriptions

## üìä Performance Expectations

- **Model Loading**: ~30-60 seconds on first startup
- **Transcription**: 2-5 seconds per minute of audio
- **Clinical Analysis**: <1 second after transcription
- **Total Processing**: 3-6 seconds for typical consultation

## üîÑ Integration Benefits

### Seamless Integration
- ‚úÖ No changes to existing endpoints
- ‚úÖ Reuses existing clinical pipeline
- ‚úÖ Maintains current data formats
- ‚úÖ Single server deployment

### Enhanced Capabilities
- ‚úÖ Voice-to-text transcription
- ‚úÖ Speaker diarization (patient/doctor)
- ‚úÖ Full clinical analysis pipeline
- ‚úÖ Multimodal voice + image support
- ‚úÖ Real-time processing capabilities

### Production Ready
- ‚úÖ Error handling and fallbacks
- ‚úÖ Model loading optimization
- ‚úÖ Health monitoring
- ‚úÖ Comprehensive logging

## üéØ Next Steps

1. **Test the integration** with your audio files
2. **Configure environment variables** for optimal performance
3. **Integrate with your frontend** to add voice recording capabilities
4. **Monitor performance** and adjust model sizes as needed
5. **Add authentication** and rate limiting for production use

## üìû Support

If you encounter any issues:

1. Check the health endpoint for model status
2. Review the logs for error messages
3. Test with the provided test script
4. Verify all dependencies are installed correctly

The integration maintains full backward compatibility with your existing API while adding powerful voice transcription capabilities!

